{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/msaxena4/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/msaxena4/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/msaxena4/.local/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/msaxena4/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/msaxena4/.local/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: jinja2 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: networkx in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch) (1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/msaxena4/.local/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/msaxena4/.local/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.4)\n",
      "Requirement already satisfied: lit in /home/msaxena4/.local/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/msaxena4/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/msaxena4/.local/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/msaxena4/.local/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/msaxena4/.local/lib/python3.10/site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/msaxena4/.local/lib/python3.10/site-packages (from gensim) (1.24.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/msaxena4/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/msaxena4/.local/lib/python3.10/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /home/msaxena4/.local/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: joblib in /home/msaxena4/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/msaxena4/.local/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/msaxena4/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/lib/python3/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/msaxena4/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/msaxena4/.local/lib/python3.10/site-packages (from scikit-learn) (1.11.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/msaxena4/.local/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/msaxena4/.local/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/msaxena4/.local/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: requests in /home/msaxena4/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/msaxena4/.local/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/msaxena4/.local/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/msaxena4/.local/lib/python3.10/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/msaxena4/.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/msaxena4/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/msaxena4/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/msaxena4/.local/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install torch\n",
    "! pip install gensim\n",
    "! pip install nltk\n",
    "! pip install scikit-learn\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/msaxena4/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/msaxena4/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/msaxena4/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-09-17 16:44:25.251069: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-17 16:44:25.795148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer  # Import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data from Json files and converting them to Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertJsontoDataframe(path):\n",
    "    # Load JSON data from the file\n",
    "    with open(path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # Create a list of dictionaries containing \"report\" and \"event_id\" keys\n",
    "    report_list = []\n",
    "    for event_report, event_id in data.items():\n",
    "        report_list.append({\"event_report\": event_report, \"event_id\": event_id})\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(report_list)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5629, 2) (2831, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = convertJsontoDataframe(\"train.json\")\n",
    "test_data = convertJsontoDataframe(\"test.json\")\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5629 entries, 0 to 5628\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   event_report  5629 non-null   object\n",
      " 1   event_id      5629 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 88.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_report</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>net lived lofty expectation month basketball l...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>duck time allowing goal game season instance s...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invezz wednesday november hewlett packard nyse...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nov photo provided george loegering large spin...</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>actress jennifer love hewitt birth baby girl n...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        event_report event_id\n",
       "0  net lived lofty expectation month basketball l...       16\n",
       "1  duck time allowing goal game season instance s...       23\n",
       "2  invezz wednesday november hewlett packard nyse...       24\n",
       "3  nov photo provided george loegering large spin...      143\n",
       "4  actress jennifer love hewitt birth baby girl n...       11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):  \n",
    "    \"\"\"\n",
    "    This function comprises of the following steps:\n",
    "    1. Lowercaseing\n",
    "    2. Tokenisation\n",
    "    3. Stopword Removal\n",
    "    4. Punctuation and Special Character removal\n",
    "    5. Lemmatization\n",
    "    6. Reconstruction\n",
    "\n",
    "    It takes in series data of text as input and returns a series of preprocessed text.\n",
    "    \"\"\"\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    for text in df:\n",
    "        text = text.lower()    # Lowercase the text    \n",
    "        tokens = word_tokenize(text)   # Tokenization\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))    \n",
    "        tokens = [word for word in tokens if word not in stop_words]    # Remove stopwords\n",
    "        tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation and special characters\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()     # Perform lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)   # Join tokens back into a string\n",
    "        preprocessed_data.append(preprocessed_text)\n",
    "\n",
    "    return pd.Series(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess train and test data\n",
    "X_train = preprocessing(train_data['event_report'])\n",
    "X_test = preprocessing(test_data['event_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    net lived lofty expectation month basketball l...\n",
       "1    duck time allowing goal game season instance s...\n",
       "2    invezz wednesday november hewlett packard nyse...\n",
       "3    nov photo provided george loegering large spin...\n",
       "4    actress jennifer love hewitt birth baby girl n...\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5629,), (2831,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'net lived lofty expectation month basketball long shot team utilized expected bench player averaging minute'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Nearest Neighbours Search Algorithm\n",
    "Note that the code is slightly modified for each of the models, since the embeddings generated from each much be handled in a different manner. Especially BERT, which uses Label encoded event_id values, which sets it apart from the other two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNearestNeighbours(k, train_embeddings, test_embeddings, y_train, y_test, model):\n",
    "    # Initialize k-Nearest Neighbors with cosine similarity\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric='cosine')   \n",
    "    knn.fit(train_embeddings)    # training the model\n",
    "\n",
    "    top_1_accuracy = []         # initialising accuracy \n",
    "    top_3_accuracy = []\n",
    "    top_5_accuracy = []\n",
    "\n",
    "    for i in range(test_data.shape[0]):\n",
    "        test_point = test_embeddings[i]\n",
    "        \n",
    "        if model == \"TF-IDF\":               # only for TF-IDF model\n",
    "            distances, indices = knn.kneighbors(test_point)\n",
    "        elif model == \"Word2Vec\":          # only for Word2Vec model\n",
    "            distances, indices = knn.kneighbors([test_point])\n",
    "        elif model == \"BERT\":              # \n",
    "            test_point = test_point.reshape(1, -1) # Reshape to (1, embedding_dim) for a single test point\n",
    "            distances, indices = knn.kneighbors(test_point)\n",
    "        \n",
    "        test_point = test_embeddings[i].reshape(1, -1)  \n",
    "        \n",
    "        if model != \"BERT\":\n",
    "            # Check event_id for the k-nearest neighbors\n",
    "            neighbor_event_ids = [train_data['event_id'].iloc[idx] for idx in indices[0]]\n",
    "\n",
    "            # Calculate and store accuracy scores\n",
    "            if test_data['event_id'].iloc[i] in neighbor_event_ids[:1]:\n",
    "                top_1_accuracy.append(1)\n",
    "            else:\n",
    "                top_1_accuracy.append(0)\n",
    "\n",
    "            if test_data['event_id'].iloc[i] in neighbor_event_ids[:3]:\n",
    "                top_3_accuracy.append(1)\n",
    "            else:\n",
    "                top_3_accuracy.append(0)\n",
    "\n",
    "            if test_data['event_id'].iloc[i] in neighbor_event_ids[:5]:\n",
    "                top_5_accuracy.append(1)\n",
    "            else:\n",
    "                top_5_accuracy.append(0)   \n",
    "                \n",
    "        else:\n",
    "            # Check event_id for the k-nearest neighbors\n",
    "            neighbor_event_ids = [y_train[idx] for idx in indices[0]]\n",
    "    \n",
    "            # Calculate and store accuracy scores\n",
    "            if y_test[i] in neighbor_event_ids[:1]:\n",
    "                top_1_accuracy.append(1)\n",
    "            else:\n",
    "                top_1_accuracy.append(0)\n",
    "    \n",
    "            if y_test[i] in neighbor_event_ids[:3]:\n",
    "                top_3_accuracy.append(1)\n",
    "            else:\n",
    "                top_3_accuracy.append(0)\n",
    "    \n",
    "            if y_test[i] in neighbor_event_ids[:5]:\n",
    "                top_5_accuracy.append(1)\n",
    "            else:\n",
    "                top_5_accuracy.append(0)      \n",
    "\n",
    "    # Calculate final accuracy scores\n",
    "    accuracy_top_1 = np.mean(top_1_accuracy)\n",
    "    accuracy_top_3 = np.mean(top_3_accuracy)\n",
    "    accuracy_top_5 = np.mean(top_5_accuracy)\n",
    "\n",
    "    print(\"Top-1 Accuracy:\", accuracy_top_1)\n",
    "    print(\"Top-3 Accuracy:\", accuracy_top_3)\n",
    "    print(\"Top-5 Accuracy:\", accuracy_top_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Baseline Approach: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k = 5\n",
      "Top-1 Accuracy: 0.7972447898269163\n",
      "Top-3 Accuracy: 0.9085128929706817\n",
      "Top-5 Accuracy: 0.9318262098198516\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_embeddings = tfidf_vectorizer.fit_transform(X_train)\n",
    "test_embeddings = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "k = 5\n",
    "# for k in range(1, 11):\n",
    "print(f'\\nk = {k}')\n",
    "KNearestNeighbours(k, train_embeddings, test_embeddings, train_data, test_data, \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Proposed Approach: Word2Vec\n",
    "\n",
    "Steps:\n",
    "1. Tokenize the preprocessed text\n",
    "2. Get embeddings for train and test data. Implement a get_sentence_embedding(sentence, model) function for this\n",
    "3. Convert the embeddings to array to use in the Nearest Neighbours Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get_sentence_embedding function\n",
    "def get_sentence_embedding(sentence, model):\n",
    "    # Initialize an empty array for the sentence embedding\n",
    "    sentence_embedding = np.zeros(model.vector_size)\n",
    "    num_words = 0\n",
    "\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            sentence_embedding += model.wv[word]\n",
    "            num_words += 1\n",
    "\n",
    "    if num_words > 0:\n",
    "        sentence_embedding /= num_words\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the preprocessed text (assuming you have the NLTK library installed)\n",
    "tokenized_train_data = [word_tokenize(text) for text in X_train]\n",
    "tokenized_test_data = [word_tokenize(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5629, 2831)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train_data), len(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['net',\n",
       " 'lived',\n",
       " 'lofty',\n",
       " 'expectation',\n",
       " 'month',\n",
       " 'basketball',\n",
       " 'long',\n",
       " 'shot',\n",
       " 'team',\n",
       " 'utilized',\n",
       " 'expected',\n",
       " 'bench',\n",
       " 'player',\n",
       " 'averaging',\n",
       " 'minute']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = tokenized_train_data, vector_size=300, window=25, min_count=2, sg=1)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the Word2Vec model\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and get embeddings for train and test data\n",
    "train_embeddings = [get_sentence_embedding(sentence, model) for sentence in tokenized_train_data]\n",
    "test_embeddings = [get_sentence_embedding(sentence, model) for sentence in tokenized_test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5629"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = np.array(train_embeddings)\n",
    "test_embeddings = np.array(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.8339809254680325\n",
      "Top-3 Accuracy: 0.9092193571176262\n",
      "Top-5 Accuracy: 0.9254680324973508\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "KNearestNeighbours(k, train_embeddings, test_embeddings, train_data, test_data, \"Word2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Proposed Approach: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps: \n",
    "1. Load BERT tokenizer and model\n",
    "2. Prepare the preprocessed data by tokenising it\n",
    "3. Perform label encoding on the \"event_id\" column\n",
    "4. Get embeddings for the input_ids from tokenised data. Implement a function get_embeddings(data_tokenized) for this.\n",
    "5. Convert these embeddings to array from and reshape it to the needed shape to run Nearest Neighbours Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and preprocess your data\n",
    "def data_preparation(data):\n",
    "    tokenized_data = [tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=128) for text in data]\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tokenized = data_preparation(X_train)\n",
    "testing_data_tokenized = data_preparation(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5658,  2973, 19459,  2100, 17626,  3204,  3455,  2146,  2915,\n",
       "          2136, 12550,  3517,  6847,  2447, 14985,  3371,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_data['event_id'])\n",
    "y_test = label_encoder.fit_transform(test_data['event_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for training and testing data\n",
    "def get_embeddings(data_tokenized):\n",
    "    embeddings = []\n",
    "    for input_ids in data_tokenized:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input_ids)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_embeddings(training_data_tokenized)\n",
    "testing_embeddings = get_embeddings(testing_data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5629"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings_array = np.array(training_embeddings)\n",
    "testing_embeddings_array = np.array(testing_embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5629, 768)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_embeddings_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings_array = training_embeddings_array.reshape(training_embeddings_array.shape[0], training_embeddings_array.shape[-1])\n",
    "testing_embeddings_array = testing_embeddings_array.reshape(testing_embeddings_array.shape[0], testing_embeddings_array.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.7997174143412222\n",
      "Top-3 Accuracy: 0.9007417873542918\n",
      "Top-5 Accuracy: 0.9272341928647121\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "KNearestNeighbours(k, training_embeddings_array, testing_embeddings_array, y_train, y_test, \"BERT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
